{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f33e3115",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.hadoop.fs.{FileSystem, Path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e64bd79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark = org.apache.spark.sql.SparkSession@5c0a40e6\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@5c0a40e6"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession.builder()\n",
    "      .appName(\"CaseStudy5 - Time-Based Data Partitioning for Ratings\")\n",
    "      .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4df596b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ratingsDataPath = gs://spark-tasks-bucket/day_16_17/rating.csv\n",
       "ratingsDF = [userId: int, movieId: int ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[userId: int, movieId: int ... 2 more fields]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ratingsDataPath = \"gs://spark-tasks-bucket/day_16_17/rating.csv\"\n",
    "val ratingsDF = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(ratingsDataPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88466b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratingsDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22089688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+-------------------+\n",
      "|userId|movieId|rating|          timestamp|\n",
      "+------+-------+------+-------------------+\n",
      "|     1|      2|   3.5|2005-04-02 23:53:47|\n",
      "|     1|     29|   3.5|2005-04-02 23:31:16|\n",
      "|     1|     32|   3.5|2005-04-02 23:33:39|\n",
      "|     1|     47|   3.5|2005-04-02 23:32:07|\n",
      "|     1|     50|   3.5|2005-04-02 23:29:40|\n",
      "|     1|    112|   3.5|2004-09-10 03:09:00|\n",
      "|     1|    151|   4.0|2004-09-10 03:08:54|\n",
      "|     1|    223|   4.0|2005-04-02 23:46:13|\n",
      "|     1|    253|   4.0|2005-04-02 23:35:40|\n",
      "|     1|    260|   4.0|2005-04-02 23:33:46|\n",
      "|     1|    293|   4.0|2005-04-02 23:31:43|\n",
      "|     1|    296|   4.0|2005-04-02 23:32:47|\n",
      "|     1|    318|   4.0|2005-04-02 23:33:18|\n",
      "|     1|    337|   3.5|2004-09-10 03:08:29|\n",
      "|     1|    367|   3.5|2005-04-02 23:53:00|\n",
      "|     1|    541|   4.0|2005-04-02 23:30:03|\n",
      "|     1|    589|   3.5|2005-04-02 23:45:57|\n",
      "|     1|    593|   3.5|2005-04-02 23:31:01|\n",
      "|     1|    653|   3.0|2004-09-10 03:08:11|\n",
      "|     1|    919|   3.5|2004-09-10 03:07:01|\n",
      "+------+-------+------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratingsDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94fb3100",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "updatedDF = [userId: int, movieId: int ... 3 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[userId: int, movieId: int ... 3 more fields]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val updatedDF = ratingsDF.withColumn(\"year\", year(col(\"timestamp\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a70c8529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+-------------------+----+\n",
      "|userId|movieId|rating|          timestamp|year|\n",
      "+------+-------+------+-------------------+----+\n",
      "|     1|      2|   3.5|2005-04-02 23:53:47|2005|\n",
      "|     1|     29|   3.5|2005-04-02 23:31:16|2005|\n",
      "|     1|     32|   3.5|2005-04-02 23:33:39|2005|\n",
      "|     1|     47|   3.5|2005-04-02 23:32:07|2005|\n",
      "|     1|     50|   3.5|2005-04-02 23:29:40|2005|\n",
      "|     1|    112|   3.5|2004-09-10 03:09:00|2004|\n",
      "|     1|    151|   4.0|2004-09-10 03:08:54|2004|\n",
      "|     1|    223|   4.0|2005-04-02 23:46:13|2005|\n",
      "|     1|    253|   4.0|2005-04-02 23:35:40|2005|\n",
      "|     1|    260|   4.0|2005-04-02 23:33:46|2005|\n",
      "|     1|    293|   4.0|2005-04-02 23:31:43|2005|\n",
      "|     1|    296|   4.0|2005-04-02 23:32:47|2005|\n",
      "|     1|    318|   4.0|2005-04-02 23:33:18|2005|\n",
      "|     1|    337|   3.5|2004-09-10 03:08:29|2004|\n",
      "|     1|    367|   3.5|2005-04-02 23:53:00|2005|\n",
      "|     1|    541|   4.0|2005-04-02 23:30:03|2005|\n",
      "|     1|    589|   3.5|2005-04-02 23:45:57|2005|\n",
      "|     1|    593|   3.5|2005-04-02 23:31:01|2005|\n",
      "|     1|    653|   3.0|2004-09-10 03:08:11|2004|\n",
      "|     1|    919|   3.5|2004-09-10 03:07:01|2004|\n",
      "+------+-------+------+-------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "updatedDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552af72a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trimmedDF = [userId: int, movieId: int ... 3 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[userId: int, movieId: int ... 3 more fields]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Has lot of records, so taking only 100000 records\n",
    "val trimmedDF = updatedDF.limit(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20000c75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "outputPath = hdfs:///user/day_16_17/case_study_5\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "hdfs:///user/day_16_17/case_study_5"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val outputPath = \"hdfs:///user/day_16_17/case_study_5\"\n",
    "trimmedDF.coalesce(1).write.partitionBy(\"year\").format(\"parquet\").mode(\"overwrite\").save(outputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8c96db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|year|\n",
      "+----+\n",
      "|1996|\n",
      "+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "path = hdfs:///user/day_16_17/case_study_5/year=1996\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "hdfs:///user/day_16_17/case_study_5/year=1996"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Try reading the data from HDFS\n",
    "\n",
    "val path = \"hdfs:///user/day_16_17/case_study_5/year=1996\"\n",
    "spark.read.option(\"basePath\", outputPath).parquet(path)\n",
    "    .select(\"year\").distinct().show()\n",
    "\n",
    "// This suggests that the data is partitioned based on userId as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c24fbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.12.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
