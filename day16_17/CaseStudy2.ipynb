{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f33e3115",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.hadoop.fs.{FileSystem, Path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e64bd79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark = org.apache.spark.sql.SparkSession@2e53ef54\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@2e53ef54"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession.builder()\n",
    "      .appName(\"CaseStudy2 - User Rating History Partitioning\")\n",
    "      .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4df596b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ratingsDataPath = gs://spark-tasks-bucket/day_16_17/rating.csv\n",
       "ratingsDF = [userId: int, movieId: int ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[userId: int, movieId: int ... 2 more fields]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ratingsDataPath = \"gs://spark-tasks-bucket/day_16_17/rating.csv\"\n",
    "val ratingsDF = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(ratingsDataPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88466b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratingsDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a6c8ec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "updatedDF = [userId: int, movieId: int ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[userId: int, movieId: int ... 2 more fields]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Filter incomplete records\n",
    "val updatedDF = ratingsDF.filter(col(\"userId\").isNotNull && col(\"userId\").isNotNull && col(\"rating\").isNotNull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5941c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-----+\n",
      "|movieId|userId|count|\n",
      "+-------+------+-----+\n",
      "+-------+------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "duplicateRatingsDF = [movieId: int, userId: int ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[movieId: int, userId: int ... 1 more field]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// // There are no duplicate ratings, if yes filter them\n",
    "\n",
    "// val duplicateRatingsDF = updatedDF\n",
    "//   .groupBy(\"movieId\", \"userId\")\n",
    "//   .count() // Count occurrences of each (movieId, userId) pair\n",
    "//   .filter(col(\"count\") > 1) // Keep only those with multiple ratings\n",
    "\n",
    "// duplicateRatingsDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c836d719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ratingsDF = [userId: int, movieId: int ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[userId: int, movieId: int ... 1 more field]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ratingsDF = updatedDF.select(\"userId\", \"movieId\", \"rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40c024e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+\n",
      "|userId|movieId|rating|\n",
      "+------+-------+------+\n",
      "|     1|      2|   3.5|\n",
      "|     1|     29|   3.5|\n",
      "|     1|     32|   3.5|\n",
      "|     1|     47|   3.5|\n",
      "|     1|     50|   3.5|\n",
      "|     1|    112|   3.5|\n",
      "|     1|    151|   4.0|\n",
      "|     1|    223|   4.0|\n",
      "|     1|    253|   4.0|\n",
      "|     1|    260|   4.0|\n",
      "|     1|    293|   4.0|\n",
      "|     1|    296|   4.0|\n",
      "|     1|    318|   4.0|\n",
      "|     1|    337|   3.5|\n",
      "|     1|    367|   3.5|\n",
      "|     1|    541|   4.0|\n",
      "|     1|    589|   3.5|\n",
      "|     1|    593|   3.5|\n",
      "|     1|    653|   3.0|\n",
      "|     1|    919|   3.5|\n",
      "+------+-------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratingsDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61bd6ab6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|userId|\n",
      "+------+\n",
      "| 81900|\n",
      "| 82529|\n",
      "| 82672|\n",
      "| 83250|\n",
      "| 83693|\n",
      "| 85321|\n",
      "| 85349|\n",
      "| 87120|\n",
      "| 87338|\n",
      "| 87462|\n",
      "| 87616|\n",
      "| 87905|\n",
      "| 90228|\n",
      "| 90550|\n",
      "| 91299|\n",
      "| 92182|\n",
      "| 92235|\n",
      "| 93319|\n",
      "| 94265|\n",
      "| 95476|\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratingsDF.select(\"userId\").distinct().show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81c3a0d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trimmedDF = [userId: int, movieId: int ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[userId: int, movieId: int ... 1 more field]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Has lot of records, so taking only 100000 records\n",
    "val trimmedDF = ratingsDF.limit(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8042e8f3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "outputPath = hdfs:///user/day_16_17/case_study_2\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "hdfs:///user/day_16_17/case_study_2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val outputPath = \"hdfs:///user/day_16_17/case_study_2\"\n",
    "trimmedDF.coalesce(1).write.partitionBy(\"userId\").format(\"parquet\").mode(\"overwrite\").save(outputPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14153e23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "36675c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|userId|\n",
      "+------+\n",
      "|     1|\n",
      "+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "path = hdfs:///user/day_16_17/case_study_2/userId=1\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "hdfs:///user/day_16_17/case_study_2/userId=1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Try reading the data from HDFS\n",
    "\n",
    "val path = \"hdfs:///user/day_16_17/case_study_2/userId=1\"\n",
    "spark.read.option(\"basePath\", outputPath).parquet(path)\n",
    "    .select(\"userId\").distinct().show()\n",
    "\n",
    "// This suggests that the data is partitioned based on userId as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c24fbbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.12.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}